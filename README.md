ğŸ¥ Confido Clinic Voice Assistant

A voice-enabled virtual receptionist built with FastAPI, Whisper (STT), ElevenLabs (TTS), and Llama3.1 (via Ollama).
It simulates a phone call experience: greeting the user, transcribing speech, generating natural responses with an LLM, and replying with synthesized voice.

ğŸš€ Features

ğŸ™ï¸ Speech-to-Text: Converts your speech into text using Whisper (faster-whisper).

ğŸ§  LLM-powered Responses: Uses Llama3.1 (Ollama) to generate natural, context-aware answers.

ğŸ”Š Text-to-Speech: Converts replies to voice with ElevenLabs API.

ğŸ“… Appointment Scheduling: Handles new or follow-up appointments with doctors.

ğŸ›¡ï¸ Insurance Queries: Answers whether specific insurance providers are accepted.

â„¹ï¸ FAQs: Provides clinic hours, location, and contact details.

ğŸŒ Web Frontend: A simple interface simulating a phone call.

ğŸ›  Installation & Setup
1. Clone the Repository
git clone https://github.com/<your-username>/confido-clinic-assistant.git
cd confido-clinic-assistant

2. Create and Activate Virtual Environment

Windows (PowerShell):
```
python -m venv venv
venv\Scripts\activate
```

macOS / Linux:
```
python3 -m venv venv
source venv/bin/activate
```
3. Install Dependencies
```
pip install -r requirements.txt
```

ğŸ”‘ API Key Configuration

Create a file named .env in the project root.

Add your ElevenLabs API key inside:
```
ELEVENLABS_API_KEY=your_api_key_here
```

ğŸ¦™ Setup Llama3.1 with Ollama
```
Install Ollama
 on your system.
Pull the Llama3.1 model:

ollama pull llama3.1
```

Test it:
```
ollama run llama3.1 "Hello, who are you?"
```
â–¶ï¸ Running the Application

Start the backend:
```
uvicorn main1:app --reload
```

Backend runs at: http://127.0.0.1:8000

Open the frontend (index.html) in your browser:

Click Start Call â†’ the AI assistant greets you.

Speak into your microphone â†’ transcription + response + synthesized voice.

ğŸ“‚ Project Structure
confido-clinic-assistant/
â”‚
â”œâ”€â”€ main1.py                # FastAPI backend
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ stt_service.py      # Speech-to-text (Whisper)
â”‚   â”œâ”€â”€ tts_service.py      # Text-to-speech (ElevenLabs)
â”‚   â”œâ”€â”€ llm_service.py      # Llama3.1 integration (Ollama)
â”‚
â”œâ”€â”€ index.html              # Web frontend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                    # Environment variables (ignored in Git)
â””â”€â”€ .gitignore

ğŸ“ Example Conversation

Assistant: Hello! Youâ€™ve reached Confido Health Clinic. How may I help you today?
User: I want to book an appointment with Dr. Singh.
Assistant: Sure! Would this be a new appointment or a follow-up?
User: A new appointment.
Assistant: Great, when would you like to schedule it?

âš ï¸ Notes

STT: faster-whisper auto-detects language, CPU/GPU fallback.

TTS: requires an ElevenLabs API key.

LLM: powered by Llama3.1 (local inference via Ollama).

Frontend: plain HTML + JS, simulates a phone call flow.
